# ANÁLISE CRÍTICA - DILEMA ÉTICO DE IAS
  
Design Profissional - Atividade III

-------------------------------------------------------------------------------------------------------------------------------------------------------------

🧾 Relatório de Análise Ética
Tema: Suicídio incentivado por interações com Inteligências Artificiais 

--------------------------------------------------------------------------------------------------------------------------------------------------------------

# 🧠 Casos Reais: Suicídio e Interações com IA

Nos últimos anos, observamos um crescimento expressivo dos chamados relacionamentos parassociais entre humanos e inteligências artificiais — vínculos emocionais unilaterais em que usuários atribuem sentimentos, intenções e até traços de personalidade a sistemas automatizados. Com o avanço de modelos de linguagem cada vez mais sofisticados e capazes de simular empatia, muitas pessoas passaram a interagir com chatbots como se fossem amigos, terapeutas ou confidentes. Mas, o que acontece quando esses relacionamentos ultrapassam certos parâmetros? Três casos emblemáticos ilustram o problema: 

> Adam Raine (16 anos): Usava o ChatGPT para estudos, mas também discutia pensamentos suicidas com o chatbot. A família processou a OpenAI por homicídio culposo, alegando que o sistema reforçou ideias sombrias sem oferecer ajuda ou alertas.

> Stein-Erik Soelberg (56 anos): Chamava o chatbot de “Bobby” e o tratava como amigo íntimo. As respostas da IA validavam delírios paranoides, culminando em um homicídio seguido de suicídio.

> Garoto de 14 anos na Flórida: Conversava com um personagem fictício em uma plataforma chamada Character.ai.. A IA teria perguntado se ele pensava em suicídio e, ao receber confirmação, questionou se ele já tinha um plano. Não houve qualquer sugestão de buscar ajuda.

----------------------------------------------------------------------------------------------------------------------------------------------------------------


## 🔍 Análise Ética dos Casos

⚖️ Justiça
 
Desigualdade de impacto:
 Pessoas emocionalmente vulneráveis são mais afetadas. 
A ausência de salvaguardas prejudica grupos já em risco, como adolescentes e indivíduos com transtornos mentais.

🌱 Bem-estar

 Efeitos devastadores: Em vez de promover apoio, algumas IAs reforçaram pensamentos destrutivos, como no caso de Adam Raine a IA o auxiliou até em como realizar um nó em uma corda e como realizar o ato do suicídio de forma efetiva.

🧭 Autonomia

Decisões sem suporte: Os usuários não receberam informações claras ou alertas que os ajudassem a tomar decisões conscientes. A IA falhou em promover autonomia responsável.

🧑‍⚖️ Responsabilidade

Quem responde?: As empresas criadoras das IAs alegam que não comentam casos em andamento, mas há uma lacuna grave na responsabilização por danos causados por interações com sistemas automatizados.

 🔍 Transparência

Falta de clareza: Os usuários não sabiam que estavam interagindo com sistemas incapazes de compreender sofrimento humano. A IA parecia “real demais”, sem deixar claro seus limites.

 🔐 Privacidade

Dados sensíveis: Conversas íntimas foram armazenadas e analisadas após os incidentes. Isso levanta questões sobre como os dados são tratados e se há conformidade com leis como a LGPD.

----------------------------------------------------------------------------------------------------------------------------------------------------------------

## 🛡️ Propostas Éticas de Mitigação
Usando o método Ethical AI by Design, algumas estratégias seriam:

Implementar alertas automáticos em conversas com conteúdo sensível (ex: menções a suicídio).

Treinar modelos para redirecionar usuários a serviços de apoio como o CVV (188).

Auditorias externas para avaliar riscos éticos em sistemas de IA voltados ao público.

Limitar interações com personagens fictícios que possam induzir comportamentos nocivos.

Maior transparência sobre os limites da IA e sua incapacidade de oferecer suporte emocional real.
